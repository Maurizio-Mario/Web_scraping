{
    "contents" : "---\ntitle: \"readLines\"\nauthor: \"Maurizio Murino\"\ndate: \"22 April 2016\"\noutput: html_document\n---\nIn this exercise I will scrape text from the rstudio blog on [web scraping](https://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/).\n\nLet's first create a connection with the website. With it, we are going import the website structure with `readLines`. Finally, we close the connection.\n\n```{r 1}\ncon <- url(\"https://blog.rstudio.org/2014/11/24/rvest-easy-web-scraping-with-r/\")\npage1 <- readLines(con)\nclose(con)\n```\n\nNow, we have a 798 lines long file. For this exercise, I will have a look to its structure open the file with a text editor. Different and more effective solutions, such as [Selector gadgets](http://selectorgadget.com/) (which is discussed in the scraped page) and nodes will be used in another excercise.\n\n```{r 2}\nlength(page1)\nwrite.table(page1, \"page1.txt\")\n```\n\nIn this phase we are interest in the text. So let's open the `page1.txt` file with a text editor. The first material of our interest is at line 117, 121, 124, 130.\n\n```{r 3}\na <- page1[c(117, 121, 124, 130)]\na\n```\n\nGotcha! Precisely what we need. Let's clean it a little. Luckly, it is quite neat.\n\n```{r 4}\na <- gsub(\"<.*?>\", \"\", a, perl = TRUE)\na <- gsub(\"\\t\\t\", \"\", a, fixed = TRUE)\na <- gsub(\"â€™\", \"\", a, fixed = TRUE)\na <- gsub(\"(&quot;selectorgadget&quot;) &#8211\", \"\", a, fixed = TRUE)\na\n```\n\nBeautiful. Now, we can load the `tm` package (Feinerer & Hornik, 2013) and convert our vector of character strings into a recognizable corpus of text using the `VectorSource` function and the `Corpus` function.\n\n```{r 5, warning=FALSE}\nlibrary(tm)\nlibrary(SnowballC)\ntxt <- VectorSource(a); rm(a)\ntxtCorpus <- Corpus(txt)\n\n# Information on the documents of the corpus\ninspect(txtCorpus)\n\n# Which id/key is assigned to every document\nmeta(txtCorpus, \"id\")\n\n# Plot the text of a document from the corpus\nwriteLines(as.character(txtCorpus[[3]]))\n\n# Print every document in the corpus\nlapply(txtCorpus, as.character)\n```\n\nNext, we make some adjustments to the text:\n\n* everything lower case; \n* removing punctuation; \n* removing numbers, \n* removing common English stop words. \n\nThe `tm map` function allows us to apply transformation functions to a corpus.\n\n```{r 6}\ntxtCorpus <- tm_map(txtCorpus, tolower)\ntxtCorpus <- tm_map(txtCorpus, removePunctuation)\ntxtCorpus <- tm_map(txtCorpus, removeNumbers)\ntxtCorpus <- tm_map(txtCorpus, removeWords, stopwords(\"english\"))\n\nwriteLines(as.character(txtCorpus[[3]]))\n```\n\nWhat an human friendly result! But we are not over yet. Next we stem the text. Stemming truncates words (e.g., “compute”, “computes” & “computing” all\nbecome “comput”). However, we need to load the `SnowballC` package (Bouchet-Valat, 2013) which allows us to identify specific stem elements using the `tm_map` function of the `tm` package.\n\n```{r 7}\ntxtCorpus <- tm_map(txtCorpus, stemDocument)\ndetach(\"package:SnowballC\")\ninspect(txtCorpus)\n```\n\nTime to remove every empty space generated by isolating the word stems in the previous step. We use the `stripWhitespace` argument of the `tm map` function to accomplish this task.\n\n```{r 8}\ntxtCorpus <- tm_map(txtCorpus, stripWhitespace)\ninspect(txtCorpus)\n```\n\nFinally, we can analyze the text. First, we create a Term Document Matrix (TDM), that is, a matrix of frequency counts for each word used in the corpus. Below we only show the first 20 words and their frequencies in each document (i.e. for us, each ‘document’ is a section from the rStudio blog).\n\nA particular mention goes to the first function. `TermDocumentMatrix` seems would have worked just fine in tm 0.5.10 but changes in tm 0.6.0 seems to have broken it. The problem is that the functions tolower and trim won't necessarily return TextDocuments (it looks like the older version may have automatically done the conversion). They instead return characters and the DocumentTermMatrix isn't sure how to handle a corpus of characters.\n\nHence, after all of your non-standard transformations (those not in getTransformations()) are done and just before the creation of the DocumentTermMatrix, you should run `tm_map(txtCorpus, PlainTextDocument)`. That should make sure all the data is in PlainTextDocument and should make DocumentTermMatrix happy.\n\n```{r 9}\ntxtCorpus2 <- tm_map(txtCorpus, PlainTextDocument)\ntdm <- TermDocumentMatrix(txtCorpus2)\ninspect(tdm[1:20, ] )\n```\n\nNext, we can begin to explore the TDM, using the `findFreqTerms` function, to find which words were used most. Below we specify that we want term / word stems which were used 2 or more times (in all documents / paragraphs).\n\n```{r 10}\nfindFreqTerms(tdm, lowfreq = 2, highfreq = Inf)\n```\n\nWith the `findAssocs` function we find words associated together. We are specifying the TDM to use, the term we want to find associates for, and the lowest acceptable correlation limit with that term. This returns a vector of terms which are associated with `\"data\"` at r = 0.60 or more, and that reports each association in descending order.\n\n```{r 11}\nfindAssocs(tdm, \"data\", corlimit = .6)\n```\nEventually, terms which occur very infrequently (i.e. sparse terms) can be removed, leaving only the ‘common’ terms. Below, the `sparse` argument refers to the maximum sparse-ness allowed for a term to be in the returned matrix; in other words, the larger the percentage, the more terms will be retained, the smaller the percentage, the fewer (but more common) terms will be retained.\n\n\n```{r 11}\ntdmCommon.2 <- removeSparseTerms(tdm, sparse = 0.20)\ntdmCommon.2\ntdmCommon.6 <- removeSparseTerms(tdm, sparse = 0.60)\ninspect(tdmCommon.2)\ninspect(tdmCommon.6)\n```\n\nWe can review the terms returned from a specific sparse-ness by using the `inspect` function with the TDMs containing those specific sparse-ness rates (i.e. the terms retained at specific spare-ness levels). \nBelow we see the 20 terms returned when sparse-ness is set to 0.60. When sparse-ness is set to 0.20 we have no term! Afterall, the data we selected were quite small.\n\nAn that is my first taxt analysis with web scraping included.\n\n\n",
    "created" : 1461310920168.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1499889124",
    "id" : "5426542B",
    "lastKnownWriteTime" : 1461324964,
    "path" : "C:/Users/MaurizioLocale/OneDrive/Data_Science/3_Getting_Cleaning_Data/Web_scraping/readLinesTxtMining.Rmd",
    "project_path" : "readLinesTxtMining.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_markdown"
}